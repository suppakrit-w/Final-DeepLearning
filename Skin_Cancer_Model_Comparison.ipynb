{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOe7s46LU5QDgG5/of6eo8D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suppakrit-w/Final-DeepLearning/blob/main/Skin_Cancer_Model_Comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Download and Prepare Data\n",
        "\n",
        "## Download Dataset"
      ],
      "metadata": {
        "id": "gADyXgQfB6TH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install Kaggle API\n",
        "!pip install -q kaggle\n",
        "\n",
        "# 2. create folder for kaggle.json\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# 3. get kaggle.json\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "# 4. Move file and set permissions\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# 5. Download Dataset\n",
        "!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000\n",
        "\n",
        "# 6. Unzip Dataset\n",
        "!unzip -q skin-cancer-mnist-ham10000.zip"
      ],
      "metadata": {
        "id": "HgAAamEF6ZPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot distribution of each class"
      ],
      "metadata": {
        "id": "WKuKM5L5B7gB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLG_Zp8D6E2H"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('HAM10000_metadata.csv')\n",
        "\n",
        "print(df['dx'].value_counts())\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "df['dx'].value_counts().plot(kind='bar')\n",
        "plt.title('Class Distribution of HAM10000')\n",
        "plt.xlabel('Lesion Type')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Move data into single folder\n",
        "\n",
        "data_sorted/  \n",
        "├── train/  \n",
        "│   ├── nv/  \n",
        "│   ├── mel/  \n",
        "│   ├── bcc/  \n",
        "│   └── ... (all 7 classes)  \n",
        "└── val/  \n",
        "    ├── nv/  \n",
        "    ├── mel/  \n",
        "    ├── bcc/  \n",
        "    └── ... (all 7 classes)  "
      ],
      "metadata": {
        "id": "0bcn-xArCKW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"Starting image organization script...\")\n",
        "\n",
        "# --- 1. Load Metadata ---\n",
        "try:\n",
        "    df = pd.read_csv('HAM10000_metadata.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'HAM10000_metadata.csv' not found.\")\n",
        "    print(\"Please make sure you have downloaded and unzipped the dataset correctly.\")\n",
        "\n",
        "# --- 2. Create Train/Val Split ---\n",
        "# We use 'stratify=df['dx']' to ensure both train and val sets\n",
        "# have the same proportion of classes as the original dataset.\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['dx']\n",
        ")\n",
        "\n",
        "print(f\"Total images: {len(df)}\")\n",
        "print(f\"Training images: {len(train_df)}\")\n",
        "print(f\"Validation images: {len(val_df)}\")\n",
        "\n",
        "# --- 3. Define Source and Destination Dirs ---\n",
        "# (MODIFIED) Define the two source folders\n",
        "source_dir_1 = 'HAM10000_images_part_1'\n",
        "source_dir_2 = 'HAM10000_images_part_2'\n",
        "base_dest_dir = 'data_sorted' # Our new organized folder\n",
        "\n",
        "# Get all 7 class names\n",
        "class_names = df['dx'].unique()\n",
        "\n",
        "# --- 4. Create Directory Structure ---\n",
        "for split in ['train', 'val']:\n",
        "    split_dir = os.path.join(base_dest_dir, split)\n",
        "    os.makedirs(split_dir, exist_ok=True)\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(split_dir, class_name)\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "print(\"Directory structure created.\")\n",
        "\n",
        "# --- 5. Function to copy files (MODIFIED) ---\n",
        "# This function now checks both source directories\n",
        "def copy_images(dataframe, split_name):\n",
        "    print(f\"\\nCopying {split_name} images...\")\n",
        "\n",
        "    for index, row in tqdm(dataframe.iterrows(), total=dataframe.shape[0]):\n",
        "        image_filename = row['image_id'] + '.jpg'\n",
        "\n",
        "        # Check source_dir_1\n",
        "        source_path_1 = os.path.join(source_dir_1, image_filename)\n",
        "        # Check source_dir_2\n",
        "        source_path_2 = os.path.join(source_dir_2, image_filename)\n",
        "\n",
        "        # Determine the correct source path\n",
        "        source_path = None\n",
        "        if os.path.exists(source_path_1):\n",
        "            source_path = source_path_1\n",
        "        elif os.path.exists(source_path_2):\n",
        "            source_path = source_path_2\n",
        "        else:\n",
        "            print(f\"Warning: Source file not found for {image_filename}\")\n",
        "            continue # Skip this file\n",
        "\n",
        "        # Define destination\n",
        "        dest_path = os.path.join(base_dest_dir, split_name, row['dx'], image_filename)\n",
        "\n",
        "        # Copy the file\n",
        "        shutil.copyfile(source_path, dest_path)\n",
        "\n",
        "# --- 6. Run the copy process ---\n",
        "copy_images(train_df, 'train')\n",
        "copy_images(val_df, 'val')\n",
        "\n",
        "print(\"\\n--- Image organization complete! ---\")\n",
        "print(f\"Data is now sorted in '{base_dest_dir}'\")"
      ],
      "metadata": {
        "id": "xKe9Lnug9sL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data pre-processing"
      ],
      "metadata": {
        "id": "3DjxsauNBizI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torchvision import datasets, transforms, models\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# === Global Constants ===\n",
        "\n",
        "# --- Hardware ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Data ---\n",
        "BASE_DIR = 'data_sorted' # โฟลเดอร์ที่จัดเรียงแล้ว\n",
        "IMAGE_SIZE = 224\n",
        "\n",
        "# --- Training ---\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 15\n",
        "LR = 0.001 # Learning Rate\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")"
      ],
      "metadata": {
        "id": "HHEU_-V9Be-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Data Pipeline ---\n",
        "\n",
        "def get_data_loaders(base_dir, batch_size, image_size):\n",
        "    \"\"\"\n",
        "    สร้างและคืนค่า DataLoaders (train, val)\n",
        "    พร้อม WeightedRandomSampler สำหรับ train_loader\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Transforms (ImageNet Stats) ---\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(15),\n",
        "            transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    # --- Datasets ---\n",
        "    train_dir = os.path.join(base_dir, 'train')\n",
        "    val_dir = os.path.join(base_dir, 'val')\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms['train'])\n",
        "    val_dataset = datasets.ImageFolder(val_dir, transform=data_transforms['val'])\n",
        "\n",
        "    class_names = train_dataset.classes\n",
        "    print(f\"Classes: {class_names}\")\n",
        "\n",
        "    # --- WeightedRandomSampler (แก้ Imbalance) ---\n",
        "    print(\"Setting up WeightedRandomSampler...\")\n",
        "    class_counts = np.bincount(train_dataset.targets)\n",
        "    class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "    sample_weights = class_weights[train_dataset.targets]\n",
        "\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights=sample_weights,\n",
        "        num_samples=len(sample_weights),\n",
        "        replacement=True\n",
        "    )\n",
        "\n",
        "    # --- DataLoaders ---\n",
        "    dataloaders = {\n",
        "        'train': DataLoader(train_dataset, batch_size=batch_size, sampler=sampler),\n",
        "        'val': DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    }\n",
        "\n",
        "    return dataloaders, class_names, len(train_dataset), len(val_dataset)"
      ],
      "metadata": {
        "id": "xeEkqA6zBn3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Create Model"
      ],
      "metadata": {
        "id": "_7cuM5eNI74C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Factory ---\n",
        "\n",
        "# Our Custom CNN\n",
        "class MyCustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(MyCustomCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # 112\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # 56\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2) # 28\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128 * 28 * 28, 512), nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# --- Model factory ---\n",
        "def create_model(model_name: str, num_classes: int, lr: float):\n",
        "    \"\"\"\n",
        "    สร้าง Model, Criterion, และ Optimizer\n",
        "    ตามชื่อโมเดลที่เลือก ('resnet', 'efficientnet', 'custom')\n",
        "    \"\"\"\n",
        "    model = None\n",
        "    optimizer = None\n",
        "\n",
        "    # Load pre-trained ResNet 50\n",
        "    if model_name == 'resnet':\n",
        "        print(\"Loading pre-trained ResNet50\")\n",
        "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "        # Freeze model parameter\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False # Freeze\n",
        "        num_ftrs = model.fc.in_features\n",
        "        # Replace ResNet fc layer\n",
        "        model.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "        optimizer = optim.Adam(model.fc.parameters(), lr=lr)\n",
        "\n",
        "    # Load pre-trained EfficientNet-B0\n",
        "    elif model_name == 'efficientnet':\n",
        "        print(\"Loading pre-trained EfficientNet-B0\")\n",
        "        model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
        "        # Freeze model parameter\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False # Freeze\n",
        "        num_ftrs = model.classifier[1].in_features\n",
        "        # Replace EffNet classifier layer\n",
        "        model.classifier = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "        optimizer = optim.Adam(model.classifier.parameters(), lr=lr)\n",
        "\n",
        "    elif model_name == 'custom':\n",
        "        print(\"Initializing Custom CNN (Baseline)\")\n",
        "        model = MyCustomCNN(num_classes=num_classes)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model name: {model_name}. Choose 'resnet', 'efficientnet', or 'custom'.\")\n",
        "\n",
        "    model = model.to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    return model, criterion, optimizer"
      ],
      "metadata": {
        "id": "Tlxdp57bBrBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Train Model"
      ],
      "metadata": {
        "id": "WsWttA0INuzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Train & Evaluate ---\n",
        "\n",
        "def train_model(model, criterion, optimizer, dataloaders, dataset_sizes, num_epochs):\n",
        "    \"\"\"\n",
        "    Main Training Loop\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "\n",
        "    # Store history for graph plotting\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\" * 10)\n",
        "\n",
        "        # -------------------\n",
        "        #  Training Phase\n",
        "        # -------------------\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "\n",
        "        for inputs, labels in tqdm(dataloaders['train'], desc=\"Training\"):\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_train_loss = running_loss / dataset_sizes['train']\n",
        "        epoch_train_acc = correct_train / dataset_sizes['train']\n",
        "\n",
        "        history['train_loss'].append(epoch_train_loss)\n",
        "        history['train_acc'].append(epoch_train_acc)\n",
        "\n",
        "        # -------------------\n",
        "        #  Validation Phase\n",
        "        # -------------------\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in tqdm(dataloaders['val'], desc=\"Validation\"):\n",
        "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = val_loss / dataset_sizes['val']\n",
        "        epoch_val_acc = correct_val / dataset_sizes['val']\n",
        "\n",
        "        history['val_loss'].append(epoch_val_loss)\n",
        "        history['val_acc'].append(epoch_val_acc)\n",
        "\n",
        "        print(f\"Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | \"\n",
        "              f\"Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    print(\"--- Finished Training ---\")\n",
        "    return model, history\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, device, class_names):\n",
        "    \"\"\"\n",
        "    Evaluate Model on Validation/Test set and print Report\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Final Evaluation ---\")\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    print(cm)\n",
        "\n",
        "    # Plot confusion matrix using seaborn\n",
        "    import seaborn as sns\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_history(history, model_name):\n",
        "    \"\"\"\n",
        "    Plot Accuracy and Loss from history\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Plotting results for {model_name} ---\")\n",
        "    epochs_range = range(1, EPOCHS + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # กราฟ Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, history['train_acc'], label='Training Accuracy')\n",
        "    plt.plot(epochs_range, history['val_acc'], label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title(f'Accuracy ({model_name})')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "\n",
        "    # กราฟ Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, history['train_loss'], label='Training Loss')\n",
        "    plt.plot(epochs_range, history['val_loss'], label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title(f'Loss ({model_name})')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "SF_w1z5GBvKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution ---\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Select Model: 'resnet', 'efficientnet', 'custom'\n",
        "MODEL_NAME_TO_RUN = 'resnet'\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "# 1. Load Dataloader\n",
        "dataloaders, class_names, train_size, val_size = get_data_loaders(\n",
        "    base_dir=BASE_DIR,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMAGE_SIZE\n",
        ")\n",
        "dataset_sizes = {'train': train_size, 'val': val_size}\n",
        "\n",
        "# 2. Create Model\n",
        "model, criterion, optimizer = create_model(\n",
        "    model_name=MODEL_NAME_TO_RUN,\n",
        "    num_classes=len(class_names),\n",
        "    lr=LR\n",
        ")\n",
        "\n",
        "# 3. Train\n",
        "model, history = train_model(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    dataloaders=dataloaders,\n",
        "    dataset_sizes=dataset_sizes,\n",
        "    num_epochs=EPOCHS\n",
        ")\n",
        "\n",
        "# 4. Plot\n",
        "plot_history(history, MODEL_NAME_TO_RUN)\n",
        "\n",
        "# 5. Evaluate\n",
        "evaluate_model(model, dataloaders['val'], DEVICE, class_names)\n",
        "\n",
        "# 6. Save model\n",
        "torch.save(model.state_dict(), f\"ham10000_{MODEL_NAME_TO_RUN}.pth\")\n",
        "print(f\"Model saved to ham10000_{MODEL_NAME_TO_RUN}.pth\")"
      ],
      "metadata": {
        "id": "vsO9WyamByVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, dataloaders['val'], DEVICE, class_names)"
      ],
      "metadata": {
        "id": "7ZKqXI-JLNvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eff-Net"
      ],
      "metadata": {
        "id": "Zuo0lK62Of8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution ---\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Select Model: 'resnet', 'efficientnet', 'custom'\n",
        "MODEL_NAME_TO_RUN = 'efficientnet'\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "# 1. Load Dataloader\n",
        "dataloaders, class_names, train_size, val_size = get_data_loaders(\n",
        "    base_dir=BASE_DIR,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMAGE_SIZE\n",
        ")\n",
        "dataset_sizes = {'train': train_size, 'val': val_size}\n",
        "\n",
        "# 2. Create Model\n",
        "model, criterion, optimizer = create_model(\n",
        "    model_name=MODEL_NAME_TO_RUN,\n",
        "    num_classes=len(class_names),\n",
        "    lr=LR\n",
        ")\n",
        "\n",
        "# 3. Train\n",
        "model, history = train_model(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    dataloaders=dataloaders,\n",
        "    dataset_sizes=dataset_sizes,\n",
        "    num_epochs=EPOCHS\n",
        ")\n",
        "\n",
        "# 4. Plot\n",
        "plot_history(history, MODEL_NAME_TO_RUN)\n",
        "\n",
        "# 5. Evaluate\n",
        "evaluate_model(model, dataloaders['val'], DEVICE, class_names)\n",
        "\n",
        "# 6. Save Model\n",
        "torch.save(model.state_dict(), f\"ham10000_{MODEL_NAME_TO_RUN}.pth\")\n",
        "print(f\"Model saved to ham10000_{MODEL_NAME_TO_RUN}.pth\")"
      ],
      "metadata": {
        "id": "axQpRhbBOfYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom CNN"
      ],
      "metadata": {
        "id": "uYS3q-emYj2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution ---\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Select Model: 'resnet', 'efficientnet', 'custom'\n",
        "MODEL_NAME_TO_RUN = 'custom'\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "# 1. Load Dataloader\n",
        "dataloaders, class_names, train_size, val_size = get_data_loaders(\n",
        "    base_dir=BASE_DIR,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMAGE_SIZE\n",
        ")\n",
        "dataset_sizes = {'train': train_size, 'val': val_size}\n",
        "\n",
        "# 2. Create Model\n",
        "model, criterion, optimizer = create_model(\n",
        "    model_name=MODEL_NAME_TO_RUN,\n",
        "    num_classes=len(class_names),\n",
        "    lr=LR\n",
        ")\n",
        "\n",
        "# 3. Train\n",
        "model, history = train_model(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    dataloaders=dataloaders,\n",
        "    dataset_sizes=dataset_sizes,\n",
        "    num_epochs=EPOCHS\n",
        ")\n",
        "\n",
        "# 4. Plot\n",
        "plot_history(history, MODEL_NAME_TO_RUN)\n",
        "\n",
        "# 5. Evaluate\n",
        "evaluate_model(model, dataloaders['val'], DEVICE, class_names)\n",
        "\n",
        "# 6. Save Model\n",
        "torch.save(model.state_dict(), f\"ham10000_{MODEL_NAME_TO_RUN}.pth\")\n",
        "print(f\"Model saved to ham10000_{MODEL_NAME_TO_RUN}.pth\")"
      ],
      "metadata": {
        "id": "Qz0unBNuWCGI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}